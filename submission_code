import pygame
import numpy as np
from typing import Tuple, List
import random
from collections import deque
import time
from tqdm import tqdm
import matplotlib.pyplot as plt
import pickle 

ACTIONS: List[str] = ['UP', 'DOWN', 'LEFT', 'RIGHT']
    
#colors for rendering
WHITE = (255, 255, 255)
BLACK = (0, 0, 0)
GREEN = (0, 255, 0)
RED = (255, 0, 0)
BLUE = (0, 0, 255)
GRAY = (200, 200, 200)

#Initialise Pygame
pygame.init()

#recreating the maze from the input file
def get_maze_from_file(filepath : str) -> List[List[str]] :
    maze = []
    file = open(filepath, 'r') #Open file in the read mode
    for line in file:
        line_removedspaces = line.strip() #pickout the white spaces from the line
        row = []
        for char in line_removedspaces:
            row.append(char)
        maze.append(row)
    return maze

maze_file = "V1.txt" #Provide the address of the maze file
maze : List[List[str]]  = get_maze_from_file(maze_file)

grid_height = len(maze)
grid_width = len(maze[0])
box_size : int = 40 #the size of the grid as x*x if x is box size
window_height = grid_height * box_size
window_width = grid_width * box_size

class maze_env :
    def __init__(self) :
        self.window = pygame.display.set_mode((window_width, window_height))
        pygame.display.set_caption("The Goblet of Fire Maze")
        self.clock = pygame.time.Clock()

    def reset(self) -> Tuple[Tuple[int, int], Tuple[int, int], Tuple[int, int]] :
        self.free_spaces : Tuple[int, int] = []
        for y in range(grid_height):
            for x in range(grid_width):
                if maze[y][x] == " ":
                    self.free_spaces.append((y,x))
        
        self.harry_pos : Tuple[int, int] = random.choice(self.free_spaces)
        self.cup_pos : Tuple[int, int] = random.choice([pos for pos in self.free_spaces if pos != self.harry_pos])
        self.death_pos : Tuple[int, int] =  random.choice([pos for pos in self.free_spaces if pos != self.harry_pos and pos != self.cup_pos])
        
        return self.get_state()

    def get_state(self) -> Tuple[Tuple[int, int], Tuple[int, int], Tuple[int, int]] :
        return (self.harry_pos, self.death_pos, self.cup_pos)
    
    def step(self, action:str) -> Tuple[Tuple[Tuple[int, int], Tuple[int, int], Tuple[int, int]], int, bool] :
        
        prev_pos = self.harry_pos #for calculating the distances

        #move harry
        self.harry_pos = self.move_harry(self.harry_pos, action)

        #move death eater
        self.death_pos = self.move_death(self.death_pos, self.harry_pos, action)

        reward : int = 0
        done : bool = False
        
        # Calculate distances
        harry_to_cup_dist = abs(self.harry_pos[0] - self.cup_pos[0]) + abs(self.harry_pos[1] - self.cup_pos[1])
        prev_harry_to_cup_dist = abs(prev_pos[0] - self.cup_pos[0]) + abs(prev_pos[1] - self.cup_pos[1])

        # Reward for getting closer to cup
        if harry_to_cup_dist < prev_harry_to_cup_dist:
            reward += 0.5
        
        if self.harry_pos == self.cup_pos :
            reward = 50
            done = True
        elif self.harry_pos == self.death_pos :
            reward = -100
            done = True
        
        return (self.get_state(), reward, done)
    
    def move_harry(self, pos: Tuple[int, int], action: str) -> Tuple[int, int] :
        (y, x) = pos
        if action == "UP" and y<grid_height-1 and maze[y+1][x] == " " :
            y+=1
        if action == "DOWN" and y>0 and maze[y-1][x] == " " :
            y-=1
        if action == "LEFT" and x>0 and maze[y][x-1] == " " :
            x-=1
        if action == "RIGHT" and x<grid_width-1 and maze[y][x+1] == " " :
            x+=1
        return (y,x)
    
    #using the bfs algorithm to move death
    def move_death(self, ipos: Tuple[int, int], target: Tuple[int, int], action: str) -> Tuple[int, int] :
        queue = deque([ipos])
        visited = set([ipos])
        parent = {}

        directions = [(1,0), (-1,0), (0,1), (0,-1)]

        while queue:
            curr = queue.popleft()
            if curr == target:
                break
            
            for dy, dx in directions:
                dy += curr[0]
                dx += curr[1]
                neighbour = (dy, dx)
                if(0<= dy <grid_height and 0<= dx <grid_width and maze[dy][dx] == " " and (dy, dx) not in visited):
                    visited.add(neighbour)
                    queue.append(neighbour)
                    parent[neighbour] = curr

        path = []
        curr = target
        while(curr != ipos):
            if curr in parent:
                path.append(curr)
                curr = parent[curr]
            else :
                return ipos
        
        if path:
            return path[-1]
        return ipos
    
    def render(self) :
        self.window.fill(BLACK)
        for y in range(grid_height):
            for x in range(grid_width):
                if maze[y][x] == " " :
                    color = WHITE
                else :
                    color = GRAY
                pygame.draw.rect(self.window, color, (x * box_size, y * box_size, box_size, box_size))
        #Green : Cup, Blue : Harry, Red : Death
        pygame.draw.rect(self.window, GREEN, (self.cup_pos[1]*box_size, self.cup_pos[0]*box_size, box_size, box_size))
        pygame.draw.rect(self.window, RED, (self.death_pos[1]*box_size, self.death_pos[0]*box_size, box_size, box_size))
        pygame.draw.rect(self.window, BLUE, (self.harry_pos[1]*box_size, self.harry_pos[0]*box_size, box_size, box_size))

        pygame.display.flip()
        time.sleep(0.2)  #Delay helps us track the maze by reducing the speed

reward_list = [] #
training_error = [] #To moniter the temporal difference across the iterations
successful_episodes : int = 0 # To moniter the number of episodes in which harry won the cup
win_rates = [] #To moniter the winrate for every 100 episodes

#Q-learning
def q_learning(env : maze_env, episodes : int = 100, render: bool = False) :
    #Render is made false to make the training faster
    learning_rate : float = 0.1
    discount_factor : float = 0.95
    epsilon : float = 1.0
    least_epsilon : float = 0.01
    epsilon_decay : float = 0.995
    global successful_episodes

    with open("q_table.pkl", "rb") as f:
            q_table = pickle.load(f)#q table for storing the q values

    consecutive_wins = 0 #variable to check the 10 consecutive wins of harry
    win_history = deque(maxlen=100) #To track the win rate across ever 100 episodes

    for ep in tqdm(range(episodes)):#tqdm to track the progress
        state = env.reset()
        done = False    
        total_reward = 0
        max_steps = 200#to avoid o and reduce the cases in which the harry and death oscillate about each other
        #like harry moves up and death moves up and viceversa for so long time
        steps = 0#to track the number of steps
    
        while not done and steps < max_steps :
            steps += 1
            if render:
                env.render()

            state_key = str(state)
            if state_key not in q_table:
                q_table[state_key] = np.zeros(len(ACTIONS))
            
            #Filtering the actionlist for every state to directly avoid the actions which lead to wall(obstacle) 
            valid_actions = [i for i, a in enumerate(ACTIONS) if env.move_harry(state[0], a) != state[0]]
            if not valid_actions:
                break

            # epsilon-greedy action selection
            if np.random.random() < epsilon:
                action_idx = random.choice(valid_actions)
            else:
                q_vals = q_table[state_key]
                masked_q_vals = np.full(len(ACTIONS), -np.inf)#set the qvalue of invalid actions of every state to -infinity
                for i in valid_actions:
                    masked_q_vals[i] = q_vals[i]
                action_idx = int(np.argmax(masked_q_vals))

            action = ACTIONS[action_idx]
            next_state, reward, done = env.step(action)

            # Reward shaping
            if next_state[0] == state[0]:  # if Harry didn't move
                reward -= 0.5 

            next_key = str(next_state)
            if next_key not in q_table:
                q_table[next_key] = np.zeros(len(ACTIONS))
            #temporal difference    
            td = reward + discount_factor * np.max(q_table[next_key]) - q_table[state_key][action_idx]
            q_table[state_key][action_idx] += learning_rate * td
            training_error.append(td) #track the training error
            total_reward += reward
            state = next_state

            if render:
                env.clock.tick(30)
        
        #For tracking the consecutive wins and the win history
        if total_reward > 0:  # Harry won
            consecutive_wins += 1
            win_history.append(1)
        else:
            consecutive_wins = 0  # Reset on loss
            win_history.append(0)
        
        #For every 100 episodes track the win rate
        if ep % 100 == 0 and ep > 0:
            win_rate = sum(win_history) / len(win_history)
            win_rates.append(win_rate)
            print(f"Episode {ep}: Last 100 episode win rate: {win_rate*100:.1f}%")

        #Track 10 consective wins
        if consecutive_wins >= 10:
            print(f"####Success! Harry won 10 consecutive episodes.")
        #Track the successful episodes
        if state[0] == state[2]:
            successful_episodes += 1

        reward_list.append(total_reward)
        #Update the value of epsilon
        epsilon = max(least_epsilon, epsilon * epsilon_decay)
    return q_table


env = maze_env()
trained_q_table = q_learning(env, episodes=100, render=False)  # render is set false for training


fig, axs = plt.subplots(ncols = 3, figsize = (12,5))
#graph the per episode rewards 
episodes = list(range(1, len(reward_list) + 1))
axs[0].plot(episodes, reward_list, color = "blue")
axs[0].set_xlabel("Episode")
axs[0].set_ylabel("Reward per episode")
axs[0].set_title("Episode Rewards")
axs[0].grid(True)
#graph the training error using the rolling length of 50 and convolving it
rolling_length = 50
axs[1].set_title("Training Errors")
training_error_moving_average = (
    np.convolve(training_error, np.ones(rolling_length), mode = "valid")/rolling_length
)
axs[1].plot(range(len(training_error_moving_average)), training_error_moving_average)
#graph the win rate per 100 episode intervals
axs[2].set_title("Win rate over time")
axs[2].set_xlabel("100-episode intervals")
axs[2].set_ylabel("Win rate")
axs[2].plot(win_rates)

plt.tight_layout
plt.show()

pygame.quit()
